% vim: tw=70
\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}

\usepackage[utf8x]{inputenc}
% \usepackage{times}
% \usepackage{url}
% \usepackage[small,bf]{caption}
% \usepackage{latexsym}
\usepackage{hyperref}

\newcommand{\ana}[1]{\texttt{#1}}
\newcommand{\f}[1]{`#1'}
\newcommand{\tool}[1]{\texttt{#1}}

\title{FST Trimming: Ending Dictionary Redundancy in Apertium} % TITLE TODO ugh
% good word: Decomposition

\name{Matthew Marting, Kevin Brubeck Unhammer}

\address{St. David's School, Kaldera språkteknologi AS \\
  Raleigh, NC., Stavanger, Norway \\
  % TODO@email.com
  ∅, unhammer+apertium@mm.st \\}

\abstract{ The Free and Open Source rule-based machine translation
  platform Apertium uses Finite State Transducers (FST's) for
  analysis, where the output of the analyser is input to a second,
  \emph{bilingual} FST. The bilingual FST is used to translate
  analysed tokens (lemmas and tags) from one language to another. We
  discuss certain problems that arise if the analyser contains entries
  that do not pass through the bilingual FST. In particular, in trying
  to avoid ``half-translated'' tokens, and avoid issues with the
  interaction between multiwords and tokenisation, language pair
  developers have created redundant copies of monolingual
  dictionaries, manually customised to fit their language pair. This
  redundancy gets in the way of sharing of data and bug fixes to
  dictionaries between language pairs. It also makes it more
  complicated to reuse dictionaries outside Apertium (e.g. in spell
  checkers). We introduce a new tool to \emph{trim} the bad entries
  from the analyser (using the bilingual FST), creating a new
  analyser. The tool is made part of
  Apertium's lttoolbox package. \\
  \newline \Keywords{FST, RBMT, dictionary-redundancy} }

\begin{document}

\maketitleabstract

\section{Introduction and background}

Apertium~\cite{forcada2011afp} is a rule-based machine translation
platform, where the data and tools are released under a Free and Open
Source license (primarily GNU GPL). Apertium translators use Finite
State Transducers (FST's) for morphological analysis, bilingual
dictionary lookup and generation of surface forms; most language
pairs\footnote{A \emph{language pair} is a set of resources to
    translate between a certain set of languages in Apertium, e.g.
Basque--Spanish.} created with Apertium use the lttoolbox FST
library for compiling XML dictionaries into binary FST's and for
processing text with such FST's. This paper discusses the problem of
redundancy in monolingual dictionaries in Apertium, and introduces a
new tool to help solve it.

The following sections give some background on how FST's fit into
Apertium, as well as the specific capabilities of lttoolbox
FST's; then we delve into the problem of monolingual and bilingual
dictionary mismatches that lead to redundant dictionary data, and
present our solution.

\subsection{FST's in the Apertium pipeline}
\label{sec:pipeline}

Translation with Apertium works as a pipeline, where each
\emph{module} processes some text and feeds its output as input to the
next module. First, a surface form like \f{fishes} passes through the
\textbf{analyser} FST module, giving a set of analyses like
\ana{fish<n><pl>/fish<vblex><pres>}, or, if it is unknown, simply
\ana{*fishes}. Tokenisation is done during analysis, letting the FST
decide in a left-right longest match fashion which words are tokens.
The compiled analyser technically contains several FST's, each marked
for whether they have entries which are tokenised in the regular
way (like regular words), or entries that may separate other tokens,
like punctuation. Anything that has an analysis is a token, and any
other sequence consisting of letters of the \texttt{alphabet} of the
analyser is an unknown word token. Anything else can separate tokens.

After analysis, one or more \textbf{disambiguation} modules select
which of the analyses is the correct one. The \textbf{pretransfer}
module does some minor formal changes to do with multiwords.

Then a disambiguated analysis like \ana{fish<n><pl>} passes through
the \textbf{bilingual} FST. Using English to Norwegian as an example,
we would get \ana{fisk<n><m><pl>} if the bilingual FST had a matching
entry, or simply \ana{@fish<n><pl>} if it was unknown in that
dictionary. So a known entry may get changes to both lemma (\ana{fish}
to \ana{fisk}) and tags (\ana{<n><pl>} to \ana{<n><m><pl>}) by the
bilingual FST. When processing input to the bilingual FST, it is
enough that the \emph{prefix} of the tag sequence matches, so a
bilingual dictionary writer can specify that \ana{fish<n>} goes to
\ana{fisk<n><m>} and not bother with specifying all inflectional tags
like number, definiteness, tense, and so on. The tag suffix (here
\ana{<pl>}) will simply be carried over.

The output of the bilingual FST is then passed to the
\textbf{structural transfer} module (which may change word order,
ensure determiner agreement, etc.), and finally a \textbf{generator}
FST which turns analyses like \ana{fisk<n><m><pl>} into forms like
\f{fiskar}. Generation is the reverse of analysis; the dictionary
which was compiled into a generator for Norwegian can also be used as
an analyser for Norwegian, by switching the compilation direction.

A major feature of the lttoolbox FST package is the support
for multiwords and compounds, and the automatic tokenisation of all
\textbf{lexical units}. A lexical unit may be
\begin{itemize}
\item a simple, non-multi-word like the noun \f{fish},
\item a space-separated word like the noun \f{hairy frogfish}, which
  will be analysed as one token even though it contains a space, but
  will otherwise have no formal
  differences from other words,
\item a multiword with \emph{inner inflection} like \f{takes out}; this is
  analysed as \ana{take<vblex><pri><p3><sg>\# out} and then, after
  disambiguation, but before bilingual dictionary lookup, turned into
  \ana{take\# out<vblex><pri><p3><sg>} -- that is, the uninflected part
  (called the $lemq$) is moved onto the lemma,
\item a token which is actually two words like \f{they'll}; this is
  analysed as \ana{prpers<prn><subj><p3><mf><pl> +will<vaux><inf>} and
  then split after disambiguation, but before bilingual dictionary
  lookup, into \ana{prpers<prn><subj><p3><mf><pl>} and
  \ana{will<vaux><inf>},
\item a combination of these three multiword types, like Catalan
  \f{creure-ho que}, analysed as \ana{creure<vblex><inf>
    +ho<prn><enc><p3><nt>\# que} and then moved and split into
  \ana{creure\# que<vblex><inf>} and \ana{ho<prn><enc><p3><nt>} after
  disambiguation, but before bilingual dictionary lookup.
\end{itemize}


In addition to the above multiwords, where the whole string is
explicitly defined as a path in the analyser FST, we have dynamically
analysed compounds which are not defined as single paths in the FST,
but still get an analysis during lookup. To mark a word as being able
to form a compound with words to the right, we give it the \f{hidden}
tag \ana{<compound-only-L>}, while a word that is able to be a
right-side of a compound (or a word on its own) gets the tag
\ana{<compound-R>}. These hidden tags are not shown in the analysis
output, but used by the FST processor during analysis. If the noun
form \f{frog} is tagged \ana{<compound-only-L>} and \f{fishes} is
tagged \ana{<compound-R>}, the lttoolbox FST processor will
analyse \f{frogfishes} as a single compound token
\ana{frog<n><sg>+fish<n><pl>} (unless the string was already in the
dictionary as an explicit token) by trying all possible ways to split
the word. After disambiguation, but before bilingual dictionary
lookup, this compound analysis is split into two tokens, so the full
word does not need to be specified in either dictionary. This feature
is very useful for e.g. Norwegian, which has very productive
compounding.

\subsection{The Problem: Redundant data}
\label{sec:problem}

Ideally, when a monolingual dictionary for, say, English is created,
that dictionary would be available for reuse unaltered (or with only
bug fixes and additions) in all language pairs where one of the
languages is English. Common data files would be factored out of
language pairs, avoiding redundancy, giving \emph{data decomposition}.
Unfortunately, that has not been the case in Apertium until recently.

If a word is in the analyser, but not in the bilingual translation
dictionary, certain difficulties arise. As the example above showed,
if \f{fishes} were unknown to both dictionaries, the output would be
\ana{*fishes}, while if it were unknown to only the second, the output
of the analyser would be \ana{@fish<n><pl>}, and of the complete
translation just \ana{@fish}. Given \f{*fishes}, a post-editor who
knows both languages can immediately see what the original was, while
the half-translated \ana{@fish} hides the inflection information in
the source text. Just lemmatising the source text – which removes
features like number, definiteness or tense – can skew meaning. For
example, if the input were the Norwegian Bokmål \f{Sonny gikk til
  hundene}, meaning ``Sonny's life went to ruin'', a Norwegian Nynorsk
translator seeing \f{Sonny gjekk til @hund} would have to look at the
larger context (or the original) to infer that it was the idiomatic
meaning, and not ``Sonny went to (a) dog''. Similarly, in the Bokmål
sentence \f{To milliarder ødela livet}, literally ``Two billions
[money] destroyed the life'', the definite form is used to mean ``his
life''; since the lemma of \f{livet} is ambiguous with the plural, the
half-translated \f{To milliarder ødela @liv} could just as well mean
``Two billions destroyed lives''.\footnote{Examples from
  \href{http://www.bt.no/bergenpuls/litteratur/Variabelt-fra-Nesbo-3081811.html}{http://www.bt.no/bergenpuls/litteratur/Variabelt-fra-Nesbo-3081811.html}
  and
  \href{http://www.nettavisen.no/2518369.html}{http://www.nettavisen.no/2518369.html}.}
But it gets worse: Some languages inflect verbs for \emph{negation},
where the half-translated lemma would hide the fact that the meaning
is negative. When translating from Turkish, if you see \f{@öl}, you
would definitely need more context or the original to know whether it
was e.g. \f{öldürdü} ``s/he/it killed'' or \f{öldürmedi} ``s/he/it did
not kill'' – if the MT is used for gisting, where the user doesn't
know the source language, this becomes particularly troublesome. For
single-token cases like this, a workaround for the post-editor is to
carry surface form information throughout the pipeline, but as we will
see below, this fails with multiwords and compounds, which are heavily
used in many Apertium language pairs.

A word not known to the bilingual FST might not have its tags
translated (or translated correctly) either. When the transfer module
tries to use the half-translated tags to determine agreement, the
\emph{context} of the half-translated word may have its meaning skewed
as well. For example, some nouns in Norwegian have only plural forms;
when translating a singular form from Northern Sámi into Norwegian,
the bilingual FST transfers the singular tag to a plural tag for such
words. Structural transfer rules insert determiners which are
inflected for number; if the noun were half-translated (thus with the
singular tag of the input), we would output a determiner with the
wrong number, thus the output has two badly translated words instead
of untranslated one.

Trying to write transfer rules to deal with half-translated tags also
\emph{increases the complexity of transfer rules}. For example, if any
noun can be missing its gender, that's one more exception to all rules
that apply gender agreement, as well as any feature that interacts
with gender.

Then there are the issues with tokenisation and multiwords. Multiwords
are entries in the dictionaries that may consist of what would
otherwise be several tokens. As an example, say you have \f{take} and
\f{out} listed in your English dictionary, and they translate fine in
isolation. When translating to Catalan, we want the phrasal verb
\f{take out} to turn into a single word \f{treure}, so we list it as a
multiword with \emph{inner inflection} in the English dictionary. This
makes any occurrence of forms of \f{take out} get a single-token
multiword analysis, e.g. \f{takes out} gets the analysis
\ana{take<vblex><pri><p3><sg>\# out}. But then the whole multiword
\emph{has} to be in the bilingual dictionary if the two words together
are to be translated. If another language pair using the same English
dictionary has both \f{take} and \f{out} in its bilingual dictionary,
but not the multiword, the individual words in isolation will be
translated, but whenever the whole string \f{take out} is seen, it
will only be lemmatised, not translated. This is both frustrating for
the language pair developer, and wasted effort in the case where we
don't \emph{need} the multiword translation.

Assuming we could live with some frustration and transfer complexity,
we could try to carry input surface forms throughout the pipeline to
at least avoid showing lemmatised forms. But here we run into another
problem. Compounds and multiwords that consist of several lemmas are
split into two units before transfer, e.g. the French contraction
\f{au} with the analysis \ana{à<pr>+le<det><def><m><sg>} turns into
\ana{à<pr>} and \ana{le<det><def><m><sg>}, while the Norwegian
compound \f{vasskokaren} analysed as \ana{vatn<n><nt><sg><ind><cmp>+
  kokar<n><m><sg><def>} turns into \ana{vatn<n><nt><sg><ind><cmp>} and
\ana{kokar<n><m><sg><def>} – compounds may also be split at more than
one point. We split the analysis so that we avoid having to list every
possible compound in the bilingual dictionary, but we can't split the
form. We don't even know which part of the form corresponds to which
part of the analysis (and of course we would not want to translate
half a word). If we attach the full form to the first part and leave
the second empty, we run into trouble if only the second part is
untranslatable, and vice versa. One hack would be to attach the form
to the first analysis and let the second analysis instead have some
special symbol signifying that it is the analysis of the form of the
previous word, e.g. \ana{vasskokaren/vatn<n><nt><sg><ind><cmp>} and
\ana{\$1/kokar<n><m><sg><def>}. But now we need the bilingual FST to
have a memory of previously seen analyses, which is a step away from
being finite-state. It would also mean that other modules which run
between the analyser and the bilingual FST in the pipeline have to be
careful to avoid changing such sequences,\footnote{E.g. the rule based
  disambiguation system Constraint Grammar allows for moving/removing
  analyses; and an experimental module for handling discontiguous
  multiwords also moves words.} introducing brittleness into an
otherwise robust system.

Due to such issues, most language pairs in Apertium have a separate
copy of each monolingual dictionary, manually \emph{trimmed} to match
the entries of the bilingual dictionary; so in the example above, if
\f{take out} did not make sense to have in the bilingual dictionary,
it would be removed from the copy of the monolingual dictionary. This
of course leads to a lot of redundancy and duplicated effort; as an
example, there are currently (as of SVN revision 50180) twelve Spanish
monolingual dictionaries in stable (SVN trunk) language pairs, with
sizes varying from 36,798 lines to 204,447 lines.

The redundancy is not limited to Spanish; in SVN trunk we also find 10
English, 7 Catalan, and 4 French dictionaries. If we include
unreleased pairs, these numbers turn to 19, 28, 8 and 16,
respectively.
In the worst case, if you add some words to an English dictionary,
there are still 27 dictionaries which miss out on your work.  The
numbers get even worse if we look at potential new language pairs.
Given 3 languages, you ``only'' need $3*(3-1)=6$ monolingual
dictionaries for all possible pairs (remember that a dictionary
provides both an analyser and a generator). But for 4 languages, you
need $4*(4-1)=12$ dictionaries; if we were to  create all possible
translation pairs of the 34 languages appearing in currently released
language pairs, we would need $34*(34-1)=1122$ monolingual dictionaries,
where 34 ought to be enough.

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{pairs-before.eps}
    \caption{Current number of monodixes with pairs of four languages}
    \label{fig:monodixes-current}
  \end{center}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{pairs-after.eps}
    \caption{Ideal number of monodixes with four languages}
    \label{fig:monodixes-ideally}
  \end{center}
\end{figure}

The lack of shared monolingual dictionaries also means that other
monolingual resources, like disambiguator data, is not shared, since
the effort of copying files is less than the effort of letting one
module depend on another for so little gain. And it complicates the reuse
of Apertium's extensive~\cite{tyers2010fosresources} set of language
resources for other systems: If you want to create a speller for some
language supported by Apertium, you either have to manually merge
dictionaries in order to gain from all the work, or (more likely) pick
the largest one and hope it's good enough.

\subsection{A Solution: Intersection}
\label{sec:solution}

However, there is a way around these troubles. Finite state machines
can be intersected with one another to produce a new finite state
machine. In the case of the Apertium transducers, what we want is to
intersect the output (or \textbf{right}) side of the full analyser
with the input (or \textbf{left}) side of the bilingual FST, producing
a \emph{trimmed} FST. We call this process \emph{trimming}.

Some recent language pairs in Apertium use the alternative, Free and
Open Source FST framework HFST~\cite{linden2011hfst}\footnote{Partly
  due to available data in that formalism, partly due to features
  missing from lttoolbox like \emph{flag diacritics}.}. Using HFST,
one can create a "prefixed" version of the bilingual FST, this is is
the concatenation of the bilingual FST and the regular expression
\ana{.*}, i.e. match any symbol zero or more times. Then the command
\tool{hfst-compose-intersect} on the analyser and the prefixed FST
creates the FST where only those paths of the analyser remain where
the right side of the analyser match the left side of the bilingual
FST. The prefixing is necessary since, as mentioned above, the
bilingual dictionary is underspecified for tag suffixes (typically
inflectional tags such as definiteness or tense, as opposed to
lemma-identifying tags such as part of speech and noun gender).

The HFST solution works, but is missing many of the
Apertium-specific features such as different types of tokenisation
FST's, and it does not handle the fact that multiwords may split or
change format before bilingual dictionary lookup. Also, unlike
lttoolbox, HFST represents compounds with an optional
transition from the end of the noun to the beginning of the noun
dictionary -- so if \ana{frog<n>} and \ana{fish<n>} were in the
analyser, but \ana{fish<n>} were missing from the bilingual FST,
\ana{frog<n>+fish<n>} would remain in the trimmed FST since the prefix
\ana{frog<n>.*} matches. In addition, using HFST in language
pairs whose data are all in lttoolbox format would introduce a
new (and rather complex) dependency both for developers, packagers and
users who compile from source.

Thus we decided to create a new tool within lttoolbox, called
\tool{lt-trim}. This tool should trim an analyser using a bilingual
FST, creating a trimmed analyser, and handle all the lttoolbox
multiwords and compounds, as well as letting us retain the special
tokenisation features of lttoolbox. The end result should be
the same as perfect manual trimming. The next section details  the
implementation of \tool{lt-trim}.\footnote{Available in the SVN
version of the lttoolbox package, see
\href{http://wiki.apertium.org/wiki/Installation}{http://wiki.apertium.org/wiki/Installation}.}


\section{Implementation of \tool{lt-trim}}

The implementation consists of two main parts: preprocessing the
bilingual dictionary, and intersecting it with the analyser. 


\subsection{Preprocessing the bilingual dictionary}

Like monolingual dictionaries, bilingual ones can actually define
several FST's, but in this case the input is already tokenised -- the
distinction is only useful for organising the source, and has no
effect on processing. So the first preprocessing step is to take the
union of these FST's. This is as simple as creating a new FST $f$,
with epsilon (empty/unlabelled) transitions from $f$'s initial state,
to each initial state in the union, and from each of their final
states to $f$'s final state.

Next, we append loopback transitions to the final state. Like
mentioned in section \ref{sec:pipeline} above, the bilingual
dictionary is underspecified for tags. We want an analyser entry
ending in \ana{<n><pl>} to match the bilingual entry ending in
\ana{<n>}. Appending loopback transitions to the final state, ie.
\ana{<n>.*}, means the intersection will end up containing
\ana{<n><pl>}; we call the bilingual dictionary \emph{prefixed} when
it has the loopback transitions appended. The next section explains
the implementation of prefixing.

The final preprocessing step is to give multiwords with inner
inflection the same format as in the analyser. As mentioned in section
\ref{sec:pipeline}, the analyser puts tags after the part of the lemma
corresponding to the inflected part, with the uninflected part of the
multiword lemma coming last.\footnote{Having the tags ``lined up
with'' or at least close to the inflection they represent eases
dictionary writing.} The bilingual dictionary has the uninflected part
before the tags, since it has to allow tag prefixes instead of
requiring full tag sequences.  Section \ref{sec:lemqmove} details how
we move the uninflected part after the (prefixed) tags in
preprocessing the bilingual dictionary.

\subsection{Prefixing the bilingual dictionary}

Apertium alphabets consist of symbol pairs, each with a left (serves
as the input in a transducer) and right (output) symbol. Both the
pairs and the symbols themselves, which can be either letters or tags,
are identified by an integer. First, the identifiers of identical
left-right pairs of the desired symbols are determined. In the case of
intersecting transducers using depth-first traversal, the method
implemented in Apertium, only the tags are desired, though the option
to include letter pairs as well still exists due to the deprecated
multiplicative method. The side from which the symbols are obtained is
also able to be specified, though in the case of prefixing a bilingual
dictionary, only the right (output) symbols are used. All of the
symbol-pairs of the given alphabet are looped-through, and depending
on which side was specified, the respective symbols are analysed. The
method differs between letters and tags. As the identifiers of
letters, which are actually the letters themselves cast to integers,
are consistent throughout all alphabets, the identifiers of letter
pairs can be directly determined. The identifiers of tags, however,
can differ, and so their individual identifiers must first be
determined before that of their respective pairs. After the
identifiers of the desired symbol pairs are determined, they are used
to create loopbacks on the bilingual dictionary using the function
$appendDotStar$. Transitions are created from the final states
of the bilingual transducer that loop directly back with each of the
identifiers.

\subsection{Moving uninflected lemma parts}
\label{sec:lemqmove}

To turn \ana{take\# out<vblex>.*} into \ana{take<vblex>.*\# out} and
so on, we do a depth-first traversal looking for an occurrence of the
\# symbol. Then we replace the \#-transition $t$ with one into the new
transducer returned by $copyWithTagsFirst(t)$. 

This function traverses the FST from the target of $t$ (in the example
above, the state before the space), building up two new transducers,
$new$ and $lemq$. We keep a $SearchState$ during traversal, which is a
pair of the current source state in the original FST and the last
added state in $lemq$. Until we see the first tag, we add all
transitions found to $lemq$, and record in the $SearchState$ which was
the last $lemq$ state we saw. In the example above, we would build up
a $lemq$ transducer containing \ana{ out} (with an initial space).
Upon seeing the first tag, we start adding transitions from the
initial state of $new$,\footnote{We keep state-mapping tables so we
can refer to the $new$ state corresponding to each original FST
state.} and don't change the last-$lemq$-state of the $SearchState$.

When reaching a final tag state $s$, we add it and the last seen
$lemq$ state $l$ to a list of pairs $f$. After the traversal is done,
we loop through each $(s,l)$ in $f$, creating a temporary copy of
$lemq$ where the $lemq$-state $l$ has been made the only final state,
and adding a \#-transition from each final tag state $s$ in $new$ into
that copy. In the example, we would copy the $lemq$ into one where the
state after the letter \ana{t} were made final, and insert that copy
after the final state $s$, the state after the \ana{<vblex>.*}. If,
from the original \#, there were a $lemq$ path that didn't have the
same last-$lemq$-state (e.g. \ana{take\# out<n>.*} or even \ana{take\#
out.*}) it would end up in a state that were not final after $s$, and
the path would not give any analyses (such paths are removed by FST
\emph{minimisation}). But if a $lemq$ path did have the same last
state, we would want it included, e.g. \ana{take\# part<vblex>.*} to
\ana{take<vblex>.*\# part}. Thus several $lemq$ paths may lead from
the \# to the various first tag states, but we only connect those
paths which were connected in the original bilingual dictionary.

Finally, we return $new$ from $copyWithTagsFirst$ and look for the
next \#.

\subsection{Intersection}

%The intersection is implemented as a depth-first traversal of the
%analyser and the bilingual FST in lockstep; that is, we only follow
%transitions which are possible in both FST's. For the possible paths,
%we create new transitions in the new, trimmed FST.

The first method we tried of intersecting FST's consisted of
multiplying them.  This is a method that is simple to prove correct;
however, it was extremely inefficient, requiring a massive amount of
memory.  First, the states of each transducer were multiplied.  This
meant that every possible state pair, consisting of a state from the
monolingual dictionary and the bilingual dictionary, was assigned a
state in the trimmed transducer.  Next, each of the transitions were
multiplied.  As the intersection is only concerned with the output of
the monolingual dictionary and the input of the bilingual dictionary,
the respective symbols had to match; though very many of them did not,
a significant number of matching symbols resulted in transitions to
redundant and unreachable states.  These were removed with
minimisation.

The tool now implements the much more efficient method of
intersection, depth-first traversal.  Both the monolingual dictionary
and the bilingual dictionary are traversed at the same time, in
lockstep; only transitions present in both are further followed and
added to the trimmed transducer.  The process is managed through
noting which states are to be processed, the pair of states currently
being processed, the next pair of states, and the states which have
already been seen.  Besides having reachable counterparts in both
transducers, a state pair will only be added to the queue if it has
not been seen yet.  However, to handle multiwords, a few other things
are necessary.

If a \ana{+} is encountered in the monolingual dictionary (indicating
a \ana{+}-type multiword) the traversal of the bilingual dictionary
resumes from its beginning. In addition, in the event that a \ana{\#}
is later encountered, the current position is recorded. The
\ana{\#}-type multiwords alone can be easily handled if the bilingual
dictionary is preprocessed to be in the same format as the monolingual
dictionary; the tags must be moved before the \ana{\#}.  However, a
combination of both \ana{+} and \ana{\#} requires that the traversal of the
bilingual dictionary return to the state at which the \ana{+} was
first encountered.

We also need some exceptions for epsilon transitions; the idea here is
that if we see an epsilon in one transducer, we move across that
epsilon without moving in the other transducer, and vice versa.
Compound symbols in the analyser are treated similarly: we add the
transitions to the trimmed analyser and move forward in the analyser
without moving forward in the bilingual FST.

To the end user (i.e. language pair developer), the tool is a simple
command line program with three arguments: the input analyser FST, the
input bilingual FST and the output trimmed analyser FST.

\section{Ending Dictionary Redundancy}

As mentioned in section \ref{sec:solution}, there are already language
pairs in Apertium that have moved to a decomposed data model, using
the HFST trimming method. At first, the HFST language
pairs would also copy dictionaries, even if they were automatically
trimmed, just to make them available for the language pair.  But over
the last year, we have created scripts for our GNU Autotools-based
build system that let a language pair have a formal dependency on one
or more monolingual data packages\footnote{This both means that source
    and compiled monolingual files are made available to the make
    files of the language pair, and that the configure script warns if
    monolingual data packages are missing. Packagers should be able to
    use this so 
    that if a user asks their package manager, e.g. \tool{apt-get}, to
    install the language pair \tool{apertium-foo-bar}, it would
automatically install dependencies \tool{apertium-foo} and
\tool{apertium-bar} first, and use files from those packages.}.  There
is now an SVN module
\texttt{languages}\footnote{\href{http://wiki.apertium.org/wiki/Languages}{http://wiki.apertium.org/wiki/Languages}}
where such monolingual data packages reside, and all of the new
HFST-based languages pairs now use such dependencies, which are
trimmed automatically, instead of making redundant dictionary copies.
Disambiguation data is also fetched from the dependency instead of
being redundantly copied.

But most of the released and ``stable'' Apertium language pairs use
lttoolbox and still have dictionary redundancy. With the new
\tool{lt-trim} tool, it is finally possible to end the
redundancy\footnote{One could argue that there is still
  \emph{cross-lingual} redundancy in the bilingual dictionaries --
  Apertium by design does not use an interlingua. Instead, the
  Apertium dictionary crossing tool
  \tool{crossdics}~\cite{toral2011crossdics-it-ca} provides ways to
  extract new translations during development: Given bilingual
  dictionaries between languages A-B and B-C, it creates a new
  bilingual dictionary between languages A-C. One argument for not
  using an interlingua during the translation process is that the
  dictionary resulting from automatic crossing needs a lot of manual
  cleaning to root out false friends, unidiomatic translations and
  other errors -- thus an interlingua would have to contain a lot more
  information than our current bilingual dictionaries in order to
  automatically disambiguate such issues. It would also require more
  linguistics knowledge of developers and heighten the entry barrier
  for new contributors.} for the pairs which use lttoolbox,
with its tokenisation, multiword and compounding features, and without
having to make those pairs dependent on a whole other FST framework
simply for compilation.

The tool has only recently been released, and there is still much work
to do in converting existing language pairs to a decomposed data
model. Monolingual dictionaries have to be merged, and the various
language pairs may have altered the tag sets in more or less subtle
ways that can affect disambiguation, transfer and other parts of the
pipeline. 

To give an example, merging the Norwegian Bokmål dictionaries of the
language pairs Northern Sámi-Bokmål and Nynorsk-Bokmål (including
changes to transfer and to the other involved dictionaries) took about
three hours of work. However, this kind of merge work happened often
in the past anyway when major changes happened to either dictionary;
from now on it can be a one-time job. Future additions and changes to
the common apertium-nob module will benefit both language pairs.

Any new languages added to Apertium can immediately reap the benefits
of the tool, without this manual merge work; this goes for many of the
in-development pairs too (e.g. the English-Norwegian language pair now
depends on the Bokmål and Nynorsk monolingual package).

%\section{Conclusion}
% TODO

\section*{Acknowledgements}
Part of the development was funded by the Google
Code-In\footnote{\href{https://code.google.com/gci/}{https://code.google.com/gci/}} programme.

\bibliographystyle{lrec2006}
\bibliography{apertium}

\end{document}
