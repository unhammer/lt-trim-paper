% vim: tw=70
\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}

% \usepackage[utf8x]{inputenc}
% \usepackage{times}
% \usepackage{url}
% \usepackage[small,bf]{caption}
% \usepackage{latexsym}
\usepackage{hyperref}

\newcommand{\ana}[1]{\texttt{#1}}
\newcommand{\f}[1]{`#1'}
\newcommand{\tool}[1]{\texttt{#1}}

\title{FST Intersection: Ending Dictionary Redundancy in Apertium} % TITLE TODO ugh
% good word: Decomposition

\name{Author1, Author2, Author3}

\address{ Affiliation1, Affiliation2, Affiliation3 \\
  Address1, Address2, Address3 \\
  author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\}

\abstract{
  A Finite State Transducer (FST) used as an analyser, whose output is
  input to another FST, may have entries that don't pass through the
  second FST. We discuss certain problems that this creates in the
  Apertium machine translation platform, and describe the development
  of a tool to \emph{trim} such entries. The tool is made part of
  Apertium's \tool{lttoolbox} package.
}

\begin{document}

\maketitleabstract

\section{Introduction and background}

Apertium~\cite{forcada2011afp} is a rule-based machine translation
platform, where the data and tools are released under a Free and Open
Source license (primarily GNU GPL). Apertium translators use Finite
State Transducers (FST's) for morphological analysis, bilingual
dictionary lookup and generation of surface forms; most language
pairs\footnote{A \emph{language pair} is a set of resources to
    translate between a certain set of languages in Apertium, e.g.
Basque--Spanish.} created with Apertium use the \tool{lttoolbox} FST
library for compiling XML dictionaries into binary FST's and for
processing text with such FST's. This paper discusses the problem of
redundancy in monolingual dictionaries in Apertium, and introduces a
new tool to help solve it.

The following sections give some background on how FST's fit into
Apertium, as well as the specific capabilities of \tool{lttoolbox}
FST's; then we delve into the problem of monolingual and bilingual
dictionary mismatches that lead to redundant dictionary data, and
present our solution.

\subsection{FST's in the Apertium pipeline}
\label{sec:pipeline}

Translation with Apertium works as a pipeline, where each
\emph{module} processes some text and feeds its output as input to the
next module. First, a surface form like \f{fishes} passes through the
\textbf{analyser} FST module, giving a set of analyses like
\ana{fish<n><pl>/fish<vblex><pres>}, or, if it is unknown, simply
\ana{*fishes}. Tokenisation is done during analysis, letting the FST
decide in a left-right longest match fashion which words are tokens.
The compiled analyser technically contains several FST's, each marked
for whether they have entries which are tokenised in the regular
way (like regular words), or entries that may separate other tokens,
like punctuation. Anything that has an analysis is a token, and any
other sequence consisting of letters of the \texttt{alphabet} of the
analyser is an unknown word token. Anything else can separate tokens.

After analysis, one or more \textbf{disambiguation} modules select
which of the analyses is the correct one. The \textbf{pretransfer}
module does some minor formal changes to do with multiwords.

Then a disambiguated analysis like \ana{fish<n><pl>} passes through
the \textbf{bilingual} FST. Using English to Norwegian as an example,
we would get \ana{fisk<n><m><pl>} if the bilingual FST had a matching
entry, or simply \ana{@fish<n><pl>} if it was unknown in that
dictionary. So a known entry may get changes to both lemma (\ana{fish}
to \ana{fisk}) and tags (\ana{<n><pl>} to \ana{<n><m><pl>}) by the
bilingual FST. When processing input to the bilingual FST, it is
enough that the \emph{prefix} of the tag sequence matches, so a
bilingual dictionary writer can specify that \ana{fish<n>} goes to
\ana{fisk<n><m>} and not bother with specifying all inflectional tags
like number, definiteness, tense, and so on. The tag suffix (here
\ana{<pl>}) will simply be carried over.

The output of the bilingual FST is then passed to the
\textbf{structural transfer} module (which may change word order,
ensure determiner agreement, etc.), and finally a \textbf{generator}
FST which turns analyses like \ana{fisk<n><m><pl>} into forms like
\f{fiskar}. Generation is the reverse of analysis; the dictionary
which was compiled into a generator for Norwegian can also be used as
an analyser for Norwegian, by switching the compilation direction.

A major feature of the \tool{lttoolbox} FST package is the support
for multiwords and compounds, and the automatic tokenisation of all
\textbf{lexical units}. A lexical unit may be
\begin{itemize}
\item a simple, non-multi-word like the noun \f{fish},
\item a space-separated word like the noun \f{hairy frogfish}, which
  will be analysed as one token even though it contains a space, but otherwise have no formal
  differences from other words,
\item a multiword with \emph{inner inflection} like \f{takes out}; this is
  analysed as \ana{take<vblex><pri><p3><sg>\# out} and then, after
  disambiguation, but before bilingual dictionary lookup, turned into
  \ana{take\# out<vblex><pri><p3><sg>} -- that is, the uninflected part
  (called the $lemq$) is moved onto the lemma,
\item a token which is actually two words like \f{they'll}; this is
  analysed as \ana{prpers<prn><subj><p3><mf><pl> +will<vaux><inf>} and
  then split after disambiguation, but before bilingual dictionary
  lookup, into \ana{prpers<prn><subj><p3><mf><pl>} and
  \ana{will<vaux><inf>},
\item a combination of these three multiword types, like Catalan
  \f{creure-ho que}, analysed as \ana{creure<vblex><inf>
    +ho<prn><enc><p3><nt>\# que} and then moved and split into
  \ana{creure\# que<vblex><inf>} and \ana{ho<prn><enc><p3><nt>} after
  disambiguation, but before bilingual dictionary lookup.
\end{itemize}


In addition to the above multiwords, where the whole string is
explicitly defined as a path in the analyser FST, we have dynamically
analysed compounds which are not defined as single paths in the FST,
but still get an analysis during lookup. To mark a word as being able
to form a compound with words to the right, we give it the \f{hidden}
tag \ana{<compound-only-L>}, while a word that is able to be a
right-side of a compound (or a word on its own) gets the tag
\ana{<compound-R>}. These hidden tags are not shown in the analysis
output, but used by the FST processor during analysis. If the noun
form \f{frog} is tagged \ana{<compound-only-L>} and \f{fishes} is
tagged \ana{<compound-R>}, the \tool{lttoolbox} FST processor will
analyse \f{frogfishes} as a single compound token
\ana{frog<n><sg>+fish<n><pl>} (unless the string was already in the
dictionary as an explicit token) by trying all possible ways to split
the word. After disambiguation, but before bilingual dictionary
lookup, this compound analysis is split into two tokens, so the full
word does not need to be specified in either dictionary. This feature
is very useful for e.g. Norwegian, which has very productive
compounding.

\subsection{The Problem: Redundant data}
\label{sec:problem}

Ideally, when a monolingual dictionary for, say, English is created,
that dictionary would be available for reuse unaltered (or with only
bug fixes and additions) in all language pairs where one of the
languages is English. Common data files would be factored out of
language pairs, avoiding redundancy, giving \emph{data decomposition}.
Unfortunately, that has not been the case in Apertium until recently.

If a word is in the analyser, but not in the bilingual translation
dictionary, certain difficulties arise. As the example above showed,
if \f{fishes} were unknown to both dictionaries, the output would be
\ana{*fishes}, while if it were unknown to only the second, the output
of the analyser would be \ana{@fish<n><pl>}, and of the complete
translation just \ana{@fish}. Given \f{*fishes}, a post-editor who
knows both languages can immediately see what the original was, while
the half-translated \ana{@fish} hides the inflection information in
the source text. Just lemmatising the source text, which removes
features like number, definiteness or tense can skew meaning.  But it
gets worse: Some languages inflect verbs for \emph{negation}, where
the half-translated lemma would hide the fact that the meaning is
negative.\footnote{For simple cases like this, a workaround is to
    carry surface form information throughout the pipeline, but it
fails with multiwords and compounds (described below), which are
heavily used in many Apertium language pairs.}

And, as mentioned above, a word not known to the bilingual FST might not
have its tags translated (or translated correctly) either; when the
transfer module tries to use the half-translated tags to determine
agreement, the \emph{context} of the half-translated word may have its
meaning skewed as well.

Trying to write transfer rules to deal with half-translated tags also
\emph{increases the complexity of transfer rules}. For example, if any
noun can be missing its gender, that's one more exception to all rules
that apply gender agreement (as well as any feature that interacts with
gender).

Finally, there are issues with tokenisation and multiwords.
Multiwords in Apertium are entries in the dictionaries that may
consist of what would otherwise be several tokens. As an example, say
you have \f{take} and \f{out} listed in your English dictionary, and
they translate fine in isolation. Now, for Catalan we want to
translate the phrasal verb \f{take out} into a single word \f{treure},
so we list it as a multiword with \emph{inner inflection} in the
English dictionary. This makes any occurrence of forms of \f{take out}
get a single-token multiword analysis, e.g. \f{takes out} gets the
analysis \ana{take<vblex><pri><p3><sg>\# out}. But then the whole multiword
\emph{has} to be in the bilingual dictionary if it is to be
translated. If another language pair using the same English dictionary
has both \f{take} and \f{out} in its bilingual dictionary, but not the
multiword, the individual words in isolation may be translated, but
whenever the whole string together is seen, it will only be
lemmatised, not translated.

Due to these issues, most language pairs in Apertium have a separate
copy of each monolingual dictionary, manually \emph{trimmed} to match
the entries of the bilingual dictionary; so in the example above, if
\f{take out} did not make sense to have in the bilingual dictionary,
it would be removed from the copy of the monolingual dictionary. This
of course leads to a lot of redundancy and duplicated effort; as an
example, there are currently (as of SVN revision 50180) twelve Spanish
monolingual dictionaries in stable (SVN trunk) language pairs, with
sizes varying from 36,798 lines to 204,447 lines.

The redundancy is not limited to Spanish; in SVN trunk we also find 10
English, 7 Catalan, and 4 French dictionaries. If we include
unreleased pairs, these numbers turn to 19, 28, 8 and 16,
respectively.
In the worst case, if you add some words to an English dictionary,
there are still 27 dictionaries which miss out on your work.  The
numbers get even worse if we look at potential new language pairs.
Given 3 languages, you ``only'' need $3*(3-1)=6$ monolingual
dictionaries for all possible pairs (remember that a dictionary
provides both an analyser and a generator). But for 4 languages, you
need $4*(4-1)=12$ dictionaries; if we were to  create all possible
translation pairs of the 34 languages appearing in currently released
language pairs, we would need $34*(34-1)=1122$ monolingual dictionaries,
where 34 ought to be enough.

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{pairs-before.eps}
    \caption{Current number of monodixes with pairs of four languages}
    \label{fig:monodixes-current}
  \end{center}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{pairs-after.eps}
    \caption{Ideal number of monodixes with four languages}
    \label{fig:monodixes-ideally}
  \end{center}
\end{figure}

The lack of shared monolingual dictionaries also means that other
monolingual resources, like disambiguator data, is not shared, since
the effort of copying files is less than the effort of letting one
module depend on another for so little gain. And it complicates the reuse
of Apertium's extensive~\cite{tyers2010fosresources} set of language
resources for other systems: If you want to create a speller for some
language supported by Apertium, you either have to manually merge
dictionaries in order to gain from all the work, or (more likely) pick
the largest one and hope it's good enough.

\subsection{A Solution: Intersection}
\label{sec:solution}

However, there is a way around these troubles. Finite state machines
can be intersected with one another to produce a new finite state
machine. In the case of the Apertium transducers, what we want is to
intersect the output (or \textbf{right}) side of the full analyser
with the input (or \textbf{left}) side of the bilingual FST, producing
a \emph{trimmed} FST. We call this process \emph{trimming}.

Some recent language pairs in Apertium use the alternative, Free and
Open Source FST framework \tool{HFST}~\cite{linden2011hfst}\footnote{Partly
due to available data in that formalism, partly due to features
missing from \tool{lttoolbox} like \emph{flag diacritics}.}. Using
\tool{HFST}, one can create a "prefixed" version of the bilingual FST, this
is is the concatenation of the bilingual FST and the regular
expression \ana{.*}, i.e. match any symbol zero or more times. Then
the command \tool{hfst-compose-intersect} on the analyser and the
prefixed FST creates the FST where only those paths of the analyser
remain where the right side of the analyser match the left side of the
bilingual FST. The prefixing is necessary since, as mentioned above,
the bilingual dictionary is underspecified for tag suffixes (typically
inflectional tags such as definiteness or tense, as opposed to
lemma-identifying tags such as part of speech and noun gender).

The \tool{HFST} solution works, but is missing many of the
Apertium-specific features such as different types of tokenisation
FST's, and it does not handle the fact that multiwords may split or
change format before bilingual dictionary lookup. Also, unlike
\tool{lttoolbox}, \tool{HFST} represents compounds with an optional
transition from the end of the noun to the beginning of the noun
dictionary -- so if \ana{frog<n>} and \ana{fish<n>} were in the
analyser, but \ana{fish<n>} were missing from the bilingual FST,
\ana{frog<n>+fish<n>} would remain in the trimmed FST since the prefix
\ana{frog<n>.*} matches. In addition, using \tool{HFST} in language
pairs whose data are all in \tool{lttoolbox} format would introduce a
new (and rather complex) dependency both for developers, packagers and
users who compile from source.

Thus we decided to create a new tool within \tool{lttoolbox}, called
\tool{lt-trim}. This tool should trim an analyser using a bilingual
FST, creating a trimmed analyser, and handle all the \tool{lttoolbox}
multiwords and compounds, as well as letting us retain the special
tokenisation features of \tool{lttoolbox}. The end result should be
the same as perfect manual trimming. The next section details  the
implementation of \tool{lt-trim}.\footnote{Available from
  \href{http://example.com/anonymized-until-peer-review}{http://example.com/anonymized-until-peer-review}.}


\section{Implementation of \tool{lt-trim}}

The implementation consists of two main parts: preprocessing the
bilingual dictionary, and intersecting it with the analyser. 


\subsection{Preprocessing the bilingual dictionary}

Like monolingual dictionaries, bilingual ones can actually define
several FST's, but in this case the input is already tokenised -- the
distinction is only useful for organising the source, and has no
effect on processing. So the first preprocessing step is to take the
union of these FST's. This is as simple as creating a new FST $f$,
with epsilon transitions from $f$'s initial state, to each initial
state in the union, and from each of their final states to $f$'s final
state.

Next, we append loopback transitions to the final state. Like
mentioned in section \ref{sec:pipeline} above, the bilingual
dictionary is underspecified for tags. We want an analyser entry
ending in \ana{<n><pl>} to match the bilingual entry ending in
\ana{<n>}. Appending loopback transitions to the final state, ie.
\ana{<n>.*}, means the intersection will end up containing
\ana{<n><pl>}. The next section explains the implementation of
loopbacks.

The final preprocessing step is to give multiwords with inner
inflection the same format as in the analyser. As mentioned in section
\ref{sec:pipeline}, the analyser puts tags after the inflected part,
and then the uninflected part of the multiword lemma. The bilingual
dictionary (because it has to allow tag prefixes instead of requiring
full tag sequences) has the uninflected part before the tags. Section
\ref{sec:lemqmove} details how we move the uninflected part after the
tags in preprocessing the bilingual dictionary.

\subsection{Prefixing the bilingual dictionary}

Apertium alphabets consist of symbol pairs, each with a left (serves
as the input in a transducer) and right (output) symbol. Both the
pairs and the symbols themselves, which can be either letters or tags,
are identified by an integer. First, the identifiers of identical
left-right pairs of the desired symbols are determined. In the case of
intersecting transducers using depth-first traversal, the method
implemented in Apertium, only the tags are desired, though the option
to include letter pairs as well still exists due to the deprecated
multiplicative method. The side from which the symbols are obtained is
also able to be specified, though in the case of prefixing a bilingual
dictionary, only the right (output) symbols are used. All of the
symbol-pairs of the given alphabet are looped-through, and depending
on which side was specified, the respective symbols are analysed. The
method differs between letters and tags. As the identifiers of
letters, which are actually the letters themselves cast to integers,
are consistent throughout all alphabets, the identifiers of letter
pairs can be directly determined. The identifiers of tags, however,
can differ, and so their individual identifiers must first be
determined before that of their respective pairs. After the
identifiers of the desired symbol pairs are determined, they are used
to create loopbacks on the bilingual dictionary using the function
$appendDotStar$. Transitions are created from the final state
of the bilingual transducer that loop directly back with each of the
identifiers.

\subsection{Moving uninflected lemma parts}
\label{sec:lemqmove}

To turn \ana{take\# out<vblex>} into \ana{take<vblex>\# out} and so on
(and for longer tag sequences too), we do a depth-first traversal looking
for an occurrence of the \# symbol. Then we replace the \#-transition
$t$ with one into the new transducer returned by
$copyWithTagsFirst(t)$. 

This function traverses the FST from the target of $t$, building up two
new transducers, $new$ and $lemq$. We keep a $SearchState$ during
traversal, which is a pair of the current source state in the original
FST and the last added state in  $lemq$. Until we see the first tag,
we add all transitions found to $lemq$, and record in the
$SearchState$
which was the last $lemq$ state we saw. Upon seeing the
first tag, we start adding transitions from the initial state of
$new$,\footnote{We keep state-mapping tables so we can refer to the
$new$ state corresponding to each original FST state.} and don't
change the $lemq$-part of the $SearchState$.

When reaching a final tag state $t$, we add it and the last seen
$lemq$ state $l$ to a list of pairs $f$. After the traversal is done,
we loop through each $(t,l)$ in $f$, creating a temporary copy of
$lemq$ where the $lemq$-state $l$ has been made the only final state,
and adding an epsilon-transition from each final tag state $t$ in
$new$ into that copy. Finally, we return $new$ from
$copyWithTagsFirst$. Thus several $lemq$ paths may lead from the \# to
the various first tag states, but we only connect those paths which
were connected in the original bilingual dictionary.

\subsection{Intersection}

The intersection implemented as a depth-first traversal of the
analyser and the bilingual FST in lockstep; that is, we only follow
transitions which are possible in both FST's. For the possible paths,
we create new transitions in the new, trimmed FST.

\section{Ending Dictionary Redundancy}

As mentioned in section \ref{sec:solution}, there are already language
pairs in Apertium that have moved to a decomposed data model, using
the \tool{HFST} trimming method. At first, the \tool{HFST} language pairs would also
copy dictionaries, even if they were automatically trimmed, just to
make them available for the language pair.  But over the last year, we
have created GNU Autotools scripts that let a language pair have a
formal dependency on one more monolingual data packages\footnote{So if
  a user asks their package manager, e.g. \tool{apt-get}, to install
  the language pair \tool{apertium-foo-bar}, it would automatically
  install dependencies \tool{apertium-foo} and \tool{apertium-bar}
  first.}. There is now an SVN module
\texttt{languages}\footnote{\href{http://wiki.apertium.org/wiki/Languages}{http://wiki.apertium.org/wiki/Languages}}
where such monolingual data packages reside, and all of the new
\tool{HFST}-based languages pairs now use such dependencies, which are
trimmed automatically, instead of making redundant dictionary copies.
Disambiguation data is also fetched from the dependency instead of
being redundantly copied.

Most of the released and ``stable'' Apertium language pairs use
\tool{lttoolbox} and still have dictionary redundancy. With the new
\tool{lt-trim} tool, it is finally possible to end the
redundancy\footnote{One could argue that there is still
  \emph{cross-lingual} redundancy in the bilingual dictionaries --
  Apertium by design does not use an interlingua. Instead, the
  Apertium dictionary crossing tool
  \tool{crossdics}~\cite{toral2011crossdics-it-ca} provides ways to
  extract new translations during development: Given bilingual
  dictionaries between languages A-B and B-C, it creates a new
  bilingual dictionary between languages A-C. One argument for not
  using an interlingua during the translation process is that the
  dictionary resulting from automatic crossing needs a lot of manual
  cleaning to root out false friends, unidiomatic translations and
  other errors -- thus an interlingua would have to contain a lot more
  information than our current bilingual dictionaries in order to
  automatically disambiguate such issues. It would also require more
  linguistics knowledge of developers and heighten the entry barrier
  for new contributors.} for the pairs which use \tool{lttoolbox},
with its tokenisation, multiword and compounding features, and without
having to make those pairs dependent on a whole other FST framework
simply for compilation.

The tool has only recently been released, and there is still much work
to do in converting existing language pairs to a decomposed data
model. Monolingual dictionaries have to be merged, and the various
language pairs may have altered the tag sets in more or less subtle
ways that can affect disambiguation, transfer and other parts of the
pipeline. However, this is a one-time job, and any new languages added
to Apertium can immediately reap the benefits.

\section*{Acknowledgements}
Part of the development was funded by the Google
Code-In\footnote{\href{https://code.google.com/gci/}{https://code.google.com/gci/}} programme.

\bibliographystyle{lrec2006}
\bibliography{apertium}

\end{document}
